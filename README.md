# ğŸ¤– WALL-E â€” Discord AI Knowledge Assistant

![Node.js](https://img.shields.io/badge/Node.js-22.x-brightgreen)
![License](https://img.shields.io/badge/License-MIT-green)
![Powered by](https://img.shields.io/badge/Powered_by-GPT--4o-blueviolet)
![Status](https://img.shields.io/badge/status-live-success)
![RAG-Powered](https://img.shields.io/badge/RAG-Powered-blueviolet)
![Embedding](https://img.shields.io/badge/Embedding-pgvector-lightblue)

WALL-E is an AI assistant. It reads documents from a GitHub repo and responds in real-time with contextual knowledge. Each user has their own memory â€” so conversations stay personal, even in shared channels.

---

<img src="https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExdWtqa3lxbGIyeTJjZTNvMDF0MmszaDdzcWNpNjV1a3B5N2R3ajBtMiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/iGJNOadhvBMuk/giphy.gif" width="500" alt="WALL-E in real life">

---

## ğŸ“š Table of Contents

- [Features](#-features)
- [File Structure](#-file-structure)
- [GitHub Integration](#-github-integration)
- [Getting Started](#-getting-started)
- [Database Schema](#-database-schema)
- [RAG (Retrieval-Augmented Generation)](#-rag-retrieval-augmented-generation)
- [How Knowledge Embedding Works](#-how-knowledge-embedding-works)
- [Built With](#-built-with)
- [Author](#-author)
- [License](#-license)

---

## ğŸš€ Features

- âœ… **Multiformat Knowledge Embedding**  
  Supports `.txt`, `.md`, `.docx`, `.pdf`, `.xlsx`, `.json`, `.html`, `.htm` `.csv`, `.yaml`, and `.yml`.

- âœ… **Contextual GPT Responses**  
  Uses both user message history and vector-matched content chunks to generate high-quality answers.

- âœ… **GitHub Knowledge Source**

  - Automatically fetches & embeds knowledge from a public GitHub repo
  - Supports periodic auto-refresh and manual `!refresh`

- âœ… **Auto-Embedding Optimization**

  - Automatically re-embeds only files that changed (SHA256 hash check)
  - Persistent embedding storage with PostgreSQL + pgvector

- âœ… **Memory**

  - Stores the latest 10 messages per user in **PostgreSQL**
  - Maintains contextual replies even across restarts
  - Memory is separated per user, even in shared channels
  - Users can clear their memory using `!reset`
  - Admins can clear memory for others using `!reset <userId>`

- âœ… **Admin Controls (prefix-based)**

  - `!bot on` / `!bot off` â€” toggle response mode
  - `!refresh` â€” manually re-embed knowledge from GitHub
  - `!change channel to <channelId>` â€” move WUNDER-AI to a new channel
  - `!reset <userId>` â€” reset any user's memory (admin-only)
  - `!set model <model name>` - changes the gpt model in use from 2 hardcoded options - gpt-3.5-turbo (cheapest) and gpt-4o (latest)
  - `!sys` â€” check system resource usage (CPU, memory, load)
  - `!usage` â€” view total messages sent and token usage per model
  - `!usage reset` â€” reset all usage and token logs

- âœ… **User Controls (prefix-based)**

  - `! <anything>` - will be ignored by the bot
  - `!files` â€” show which files were referenced
  - `!reset` â€” clear your own conversation history
  - `!source` â€” show the text chunks used in the last reply

- âœ… **System Prompt (Custom Personality)**
  - Stored as `system-prompt.txt` inside `walle-config/` in your GitHub knowledge repo
  - Fetched via GitHub API and decoded from base64
  - Auto-refreshes every 10 min + manual `!refresh`
  - Fully editable via GitHub â€” no restart or redeploy needed
  - Guides tone, behavior, and assistant rules (not part of embedded knowledge)

---

## ğŸ“‚ File Structure

```
ğŸ“ WALL-E/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ initializeBotData.js        # Startup embedding + auto-refresh
â”‚   â”œâ”€â”€ messageMemory.js            # In-memory chat context tracking
â”‚   â”œâ”€â”€ fetchOpenAIResponse.js      # Retry-safe OpenAI call
â”‚   â”œâ”€â”€ buildSystemPrompt.js        # Prompt builder from context chunks
â”‚   â”œâ”€â”€ permissions.js              # Role checker
â”‚   â”œâ”€â”€ typing.js                   # Typing animation util
â”œâ”€â”€ commands/
â”‚   â”œâ”€â”€ adminCommands.js            # !bot off, !refresh, etc.
â”‚   â”œâ”€â”€ infoCommands.js             # !source, !files
â”œâ”€â”€ events/
â”‚   â””â”€â”€ onMessageCreate.js          # Main message handler
â”œâ”€â”€ githubFileLoader.js             # GitHub file fetching + parsing
â”œâ”€â”€ knowledgeEmbedder.js            # Embedding, pgvector storage, matching
â”œâ”€â”€ db.js                           # PostgreSQL connection + vector logic
â”œâ”€â”€ server.js                       # Express + Discord init
â”œâ”€â”€ index.html                      # Web UI for remote send
â”œâ”€â”€ .env                            # Environment variables (do not commit this)
â”œâ”€â”€ package.json
```

---

## ğŸŒ GitHub Integration

- WALL-E pulls both **knowledge files** and the **system prompt** from a private GitHub repo
- Files in the **repo root** are embedded into vector memory if supported
- The file `walle-config/system-prompt.txt` defines the botâ€™s tone, rules, and behavior
  - It is fetched via GitHub API and decoded at runtime
  - Not part of embedded knowledge â€” used only as system instructions
  - Auto-refreshes every 10 minutes, or immediately via `!refresh`

---

## ğŸ› ï¸ Getting Started

### A `.env` file is needed

```env
DISCORD_TOKEN=your_discord_token
OPENAI_KEY=your_openai_key
GITHUB_TOKEN=your_github_token
REPO_API_URL=https://api.github.com/repos/reinisvaravs/discord-bot-test-info/contents/
PROMPT_PATH_URL=https://api.github.com/repos/reinisvaravs/wall-e-info/contents/walle-config/system-prompt.txt
DATABASE_URL=postgres://username:password@host:port/db
RENDER=true
```

> âš ï¸ **Important:** Do not commit `.env` to GitHub. Add it to `.gitignore`.

## ğŸ§± Database Schema

These are the required PostgreSQL tables used by WALL-E for persistent memory, embedding storage, and usage tracking. Run them in order after creating your NeonDB instance.

> ğŸ’¡ **Make sure `pgvector` is enabled first**  
> Run this before creating any tables:
>
> ```sql
> CREATE EXTENSION IF NOT EXISTS vector;
> ```

---

### 1. `bot_config`

Stores general configuration as key-value pairs.

```sql
CREATE TABLE bot_config (
  key TEXT PRIMARY KEY,
  value TEXT NOT NULL
);
```

---

### 2. `file_hashes`

Tracks the last known hash of each embedded file to avoid redundant processing.

```sql
CREATE TABLE file_hashes (
  file_name TEXT PRIMARY KEY,
  hash TEXT NOT NULL
);
```

---

### 3. `vectors`

Stores chunked content embeddings for vector search.

```sql
CREATE TABLE vectors (
  id SERIAL PRIMARY KEY,
  file_name TEXT NOT NULL,
  chunk TEXT NOT NULL,
  embedding vector(1536),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

### 4. `user_memory`

Stores message memory for each user (used in chat context).

```sql
CREATE TABLE user_memory (
  user_id TEXT PRIMARY KEY,
  memory JSONB
);
```

---

### 5. `user_logs`

Tracks token usage per model and user (useful for analytics or billing).

```sql
CREATE TABLE user_logs (
  id SERIAL PRIMARY KEY,
  user_id TEXT,
  model TEXT,
  tokens INTEGER,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

### 6. `bot_stats`

```sql
CREATE TABLE bot_stats (
  stat_key TEXT PRIMARY KEY,
  value INTEGER
);
```

---

## ğŸ§  RAG (Retrieval-Augmented Generation)

WALL-E uses a **Retrieval-Augmented Generation (RAG)** architecture to deliver highly relevant and context-aware answers:

- ğŸ“„ **Document Ingestion**: Files from a GitHub repository are parsed and chunked for semantic relevance.
- ğŸ§¬ **Embedding**: Chunks are embedded using OpenAIâ€™s `text-embedding-3-small` model and stored in PostgreSQL with pgvector.
- ğŸ” **Retrieval**: When a user sends a message, WALL-E retrieves the most relevant chunks from the vector database using similarity search.
- ğŸ’¬ **Generation**: Retrieved chunks are included in the GPT prompt to ground the response in accurate knowledge.
- âš™ï¸ **Efficiency**: Re-embedding only occurs when file content changes (via SHA256 hash check), optimizing both speed and token costs.

This setup ensures the bot answers with **real knowledge**, not hallucinated guesses â€” perfect for technical documentation, FAQs, or internal tools.

---

## ğŸ§  How Knowledge Embedding Works

- At startup, WALL-E fetches all files from GitHub
- Computes SHA256 hash of each file to detect changes
- Skips unchanged files by checking their SHA256 hash
- Embeds changed chunks using `text-embedding-3-small`
- Stores results in a PostgreSQL table with `pgvector`
- Top matched chunks are retrieved using vector similarity for GPT replies

---

## ğŸ§° Built With

- [discord.js](https://discord.js.org/)
- [OpenAI SDK](https://www.npmjs.com/package/openai)
- [pg](https://node-postgres.com/)
- [pgvector](https://github.com/pgvector/pgvector-node)
- [mammoth](https://github.com/mwilliamson/mammoth.js)
- [pdfjs-dist](https://github.com/mozilla/pdf.js)
- [xlsx](https://www.npmjs.com/package/xlsx)
- [express](https://expressjs.com/)
- [dotenv](https://www.npmjs.com/package/dotenv)

---

## ğŸ§  Author

**Reinis Roberts VÄravs**
Latvia ğŸ‡±ğŸ‡» | Full-stack Developer
ğŸŒ [Portfolio site](https://reinisvaravs.com)
ğŸ”— [GitHub](https://github.com/reinisvaravs)

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).  
You're free to use, modify, and distribute it with attribution. Contributions are welcome!
